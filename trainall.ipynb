{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 1. Import thư viện"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-25T12:23:58.495030Z","iopub.status.busy":"2024-09-25T12:23:58.494308Z","iopub.status.idle":"2024-09-25T12:24:03.729522Z","shell.execute_reply":"2024-09-25T12:24:03.728566Z","shell.execute_reply.started":"2024-09-25T12:23:58.494992Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torchsummary import summary\n","from torch.utils.data import DataLoader, Dataset\n","import cv2\n","import numpy as np\n","import os\n","from PIL import Image\n","from tqdm import tqdm\n","import time\n","import matplotlib.pyplot as plt\n","from models.hqsr import *"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-25T12:24:03.731452Z","iopub.status.busy":"2024-09-25T12:24:03.730941Z","iopub.status.idle":"2024-09-25T12:24:03.794823Z","shell.execute_reply":"2024-09-25T12:24:03.793839Z","shell.execute_reply.started":"2024-09-25T12:24:03.731417Z"},"trusted":true},"outputs":[],"source":["import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Tạo Mô hình SR"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-25T12:24:03.824325Z","iopub.status.busy":"2024-09-25T12:24:03.823331Z","iopub.status.idle":"2024-09-25T12:24:03.833160Z","shell.execute_reply":"2024-09-25T12:24:03.832215Z","shell.execute_reply.started":"2024-09-25T12:24:03.824292Z"},"trusted":true},"outputs":[],"source":["class ImageDataset(Dataset):\n","    def __init__(self, lr_dir, hr_dir, scale, valid = False):\n","        self.lr_files = sorted(os.listdir(lr_dir))\n","        self.hr_files = sorted(os.listdir(hr_dir))\n","        self.lr_dir = lr_dir\n","        self.hr_dir = hr_dir\n","        self.scale = scale\n","        self.valid = valid\n","\n","    def __len__(self):\n","        return len(self.lr_files)\n","\n","    def __getitem__(self, idx):\n","        lr_image = Image.open(os.path.join(self.lr_dir, self.lr_files[idx])).convert('RGB')\n","        hr_image = Image.open(os.path.join(self.hr_dir, self.hr_files[idx])).convert('RGB')\n","    \n","        w, h= hr_image.size\n","        if self.valid:\n","            lr_image = lr_image.resize((w//self.scale, h//self.scale))\n","        transform = transforms.Compose([\n","            transforms.ToTensor()\n","        ])\n","        \n","        lr_image = transform(lr_image)\n","        hr_image = transform(hr_image)\n","        return lr_image, hr_image"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Tạo Hyperparameter"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-25T12:24:03.835254Z","iopub.status.busy":"2024-09-25T12:24:03.834750Z","iopub.status.idle":"2024-09-25T12:24:03.841602Z","shell.execute_reply":"2024-09-25T12:24:03.840600Z","shell.execute_reply.started":"2024-09-25T12:24:03.835180Z"},"trusted":true},"outputs":[],"source":["# Đường dẫn tới bộ dữ liệu\n","\n","# test_hr_dir  = '/kaggle/input/srdataset/sr_data/test/HR'\n","# test_lr_dir  = '/kaggle/input/srdataset/sr_data/test/LR'\n","\n","# print(torch.cuda.memory_allocated())\n","# print(torch.cuda.memory_reserved())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_psnr(img1, img2):\n","    mse = torch.mean((img1 - img2) ** 2)\n","    if mse == 0:\n","        return float('inf')\n","    max_pixel = 1.0\n","    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n","    return psnr.item()"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-09-25T12:33:32.155872Z","iopub.status.busy":"2024-09-25T12:33:32.154884Z","iopub.status.idle":"2024-09-25T14:57:54.133761Z","shell.execute_reply":"2024-09-25T14:57:54.132406Z","shell.execute_reply.started":"2024-09-25T12:33:32.155839Z"},"trusted":true},"outputs":[],"source":["# from torch.amp import autocast, GradScaler\n","# from torchsummary import summary\n","# scaler = GradScaler()\n","\n","# # Khởi tạo dataset và dataloader\n","# for scale in [2, 3, 4]:\n","#     train_lr_dir = f'dataset/Train/LR_{scale}'\n","#     train_hr_dir = 'dataset/Train/HR'\n","#     valid_lr_dir = 'dataset/Test/HR'\n","#     valid_hr_dir = 'dataset/Test/HR'\n","#     train_dataset = ImageDataset(train_lr_dir, train_hr_dir, scale=scale)\n","#     train_loader = DataLoader(train_dataset, batch_size = 16, shuffle=True)\n","\n","#     valid_dataset = ImageDataset(valid_lr_dir, valid_hr_dir, scale=scale, valid=True)\n","#     valid_loader = DataLoader(valid_dataset)\n","\n","#     # print(len(train_loader))\n","#     # Khởi tạo mô hình, loss function và optimizer\n","#     torch.cuda.empty_cache()\n","\n","#     sobelsr = HQSR(scale_factor = scale, use_sobel = True).to(device)\n","#     # sobelsr.load_state_dict(torch.load('weight/best_sobel_srx4_model.pth', map_location=device))\n","#     criterion = nn.MSELoss()\n","#     optim_sobel = optim.Adam(sobelsr.parameters(), lr=1e-4,betas =(0.9, 0.999))\n","#     scheduler_sobel = optim.lr_scheduler.StepLR(optim_sobel, step_size=10**5, gamma=0.5)\n","#     # summary(sobelsr.cuda(), input_size=(3, 510, 339), device='cuda')\n","#     cannysr = HQSR(scale_factor = scale, use_canny = True).to(device)\n","#     # cannysr = nn.DataParallel(cannysr).to(device)\n","#     # cannysr.load_state_dict(torch.load('weight/best_canny_srx4_model.pth', map_location=device))\n","    \n","#     optim_canny = optim.Adam(cannysr.parameters(), lr=1e-4,betas =(0.9, 0.999))\n","#     scheduler_canny = optim.lr_scheduler.StepLR(optim_canny, step_size=10**5, gamma=0.5)\n","#     num_epochs = 24\n","\n","#     best_psnr_sobel = float('-inf')\n","#     best_psnr_canny = float('-inf')\n","#     torch.cuda.empty_cache()\n","\n","#     losses_sobel = []\n","#     losses_canny = []\n","#     avg_psnr_sobel = []\n","#     avg_psnr_canny = []\n","\n","#     val_avg_psnr_sobel = []  # Validation PSNR\n","#     val_avg_psnr_canny = []\n","\n","#     patience = 5\n","#     epochs_no_improve = 0\n","#     log_file = open('outputs/train_log/hqsr.txt', 'a')\n","#     scaler = GradScaler()\n","\n","#     for epoch in range(num_epochs):\n","#         sobelsr.train()\n","#         cannysr.train()\n","\n","#         epoch_loss_sobel = 0\n","#         psnr_values_sobel = 0\n","#         epoch_loss_canny = 0\n","#         psnr_values_canny = 0\n","#         start_time = time.time()\n","\n","#         # Training loop\n","#         # for (lr_images, hr_images) in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch'):\n","#         #     lr_images = lr_images.cuda()\n","#         #     hr_images = hr_images.cuda()\n","\n","#         #     # Sobel SR training\n","#         #     optim_sobel.zero_grad()  \n","#         #     with autocast(device_type='cuda'):\n","#         #         outputs_sobel = sobelsr(lr_images)\n","#         #         loss_sobel = criterion(outputs_sobel, hr_images)\n","#         #     psnr_sobel = calculate_psnr(outputs_sobel, hr_images)\n","                \n","#         #     scaler.scale(loss_sobel).backward()\n","#         #     scaler.step(optim_sobel)\n","#         #     scaler.update()\n","#         #     scheduler_sobel.step()\n","\n","#         #     # Canny SR training\n","#         #     optim_canny.zero_grad()  \n","#         #     with autocast(device_type='cuda'):\n","#         #         outputs_canny = cannysr(lr_images)\n","#         #         loss_canny = criterion(outputs_canny, hr_images)\n","#         #     psnr_canny = calculate_psnr(outputs_canny, hr_images)\n","\n","#         #     scaler.scale(loss_canny).backward()\n","#         #     scaler.step(optim_canny)\n","#         #     scaler.update()\n","#         #     scheduler_canny.step()\n","            \n","#         #     # Update metrics\n","#         #     epoch_loss_sobel += loss_sobel.item()\n","#         #     psnr_values_sobel += psnr_sobel\n","#         #     epoch_loss_canny += loss_canny.item()\n","#         #     psnr_values_canny += psnr_canny\n","\n","#         # Calculate average training metrics\n","#         avg_epoch_loss_sobel = epoch_loss_sobel / len(train_loader)\n","#         average_psnr_sobel = psnr_values_sobel / len(train_loader)\n","#         losses_sobel.append(avg_epoch_loss_sobel)\n","#         avg_psnr_sobel.append(average_psnr_sobel)\n","\n","#         avg_epoch_loss_canny = epoch_loss_canny / len(train_loader)\n","#         average_psnr_canny = psnr_values_canny / len(train_loader)\n","#         losses_canny.append(avg_epoch_loss_canny)\n","#         avg_psnr_canny.append(average_psnr_canny)\n","\n","#         # Validation step\n","#         sobelsr.eval()\n","#         cannysr.eval()\n","\n","#         val_psnr_values_sobel = 0\n","#         val_psnr_values_canny = 0\n","\n","#         with torch.no_grad():  # No gradients during validation\n","#             for (lr_images, hr_images) in tqdm(valid_loader, desc=f'Validation Epoch {epoch+1}/{num_epochs}', unit='batch'):\n","#                 lr_images = lr_images.cuda()\n","#                 hr_images = hr_images.cuda()\n","\n","#                 # Sobel SR validation (no loss, only PSNR)\n","#                 outputs_sobel = sobelsr(lr_images)\n","#                 psnr_sobel = calculate_psnr(outputs_sobel, hr_images)\n","\n","#                 # Canny SR validation (no loss, only PSNR)\n","#                 outputs_canny = cannysr(lr_images)\n","#                 psnr_canny = calculate_psnr(outputs_canny, hr_images)\n","\n","#                 # Update validation PSNR\n","#                 val_psnr_values_sobel += psnr_sobel\n","#                 val_psnr_values_canny += psnr_canny\n","\n","#         # Calculate average validation PSNR\n","#         val_average_psnr_sobel = val_psnr_values_sobel / len(valid_loader)\n","#         val_avg_psnr_sobel.append(val_average_psnr_sobel)\n","\n","#         val_average_psnr_canny = val_psnr_values_canny / len(valid_loader)\n","#         val_avg_psnr_canny.append(val_average_psnr_canny)\n","\n","#         end_time = time.time()\n","\n","#         # Logging results\n","#         log_string = (f\"Epoch {epoch+1}/{num_epochs}, Loss sobel: {avg_epoch_loss_sobel:.4f}, \"\n","#                     f\"Loss canny: {avg_epoch_loss_canny:.4f}, Time training: {end_time - start_time:.4f}s, \"\n","#                     f\"PSNR sobel: {average_psnr_sobel:.2f} dB, PSNR canny: {average_psnr_canny:.2f} dB, \"\n","#                     f\"Val PSNR sobel: {val_average_psnr_sobel:.2f} dB, Val PSNR canny: {val_average_psnr_canny:.2f} dB\")\n","#         print(log_string)\n","#         log_file.write(log_string + '\\n')\n","#         log_file.flush()\n","\n","#         # Save best models based on validation PSNR\n","#         if val_average_psnr_sobel > best_psnr_sobel:\n","#             best_psnr_sobel = val_average_psnr_sobel\n","#             torch.save(sobelsr.state_dict(), f'outputs/weight_sr/x{scale}/best_hqsr_sobel.pth')\n","#             print(f\"Saved Sobel SR model with PSNR {best_psnr_sobel:.4f}\")\n","#             epochs_no_improve=0\n","        \n","\n","#         if val_average_psnr_canny > best_psnr_canny:\n","#             best_psnr_canny = val_average_psnr_canny\n","#             torch.save(cannysr.state_dict(), f'outputs/weight_sr/x{scale}/best_hqsr_canny.pth')\n","#             print(f\"Saved Canny SR model with PSNR {best_psnr_canny:.4f}\")\n","#             epochs_no_improve=0\n","        \n","#         if (val_average_psnr_sobel < best_psnr_sobel) and (val_average_psnr_canny < best_psnr_canny):\n","#             epochs_no_improve+=1\n","#         if epochs_no_improve >= patience:\n","#             print(f\"PSNR did not improve for 50 epochs. Early stopping at epoch {epoch+1}\")\n","#             break\n","#         # Clear cache and optionally save models at each epoch\n","#         save_dir = f'outputs/path/x{scale}'\n","#         if not os.path.exists(save_dir):\n","#             os.makedirs(save_dir)\n","\n","#         torch.save(sobelsr.state_dict(), os.path.join(save_dir, f'hqsr_sobel_{epoch}.pth'))\n","#         torch.save(cannysr.state_dict(), os.path.join(save_dir, f'hqsr_canny_{epoch}.pth'))\n","#             # Close log file after training\n","#     log_file.close()\n","\n","#     # Plotting results\n","#     plt.figure(figsize=(12, 10))\n","\n","#     # Plot loss\n","#     plt.subplot(2, 1, 1)\n","#     plt.plot(losses_sobel, label='Sobel SR Loss (Train)')\n","#     plt.plot(losses_canny, label='Canny SR Loss (Train)')\n","#     plt.xlabel('Epoch')\n","#     plt.ylabel('Loss')\n","#     plt.legend()\n","#     plt.title('Training Loss')\n","\n","#     # Plot PSNR\n","#     plt.subplot(2, 1, 2)\n","#     plt.plot(avg_psnr_sobel, label='Sobel SR PSNR (Train)')\n","#     plt.plot(val_avg_psnr_sobel, label='Sobel SR PSNR (Val)')\n","#     plt.plot(avg_psnr_canny, label='Canny SR PSNR (Train)')\n","#     plt.plot(val_avg_psnr_canny, label='Canny SR PSNR (Val)')\n","#     plt.xlabel('Epoch')\n","#     plt.ylabel('PSNR (dB)')\n","#     plt.legend()\n","#     plt.title('Average PSNR (Train and Val)')\n","\n","#     plt.tight_layout()\n","#     plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# E2DSR"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","from PIL import Image\n","import shutil\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.parallel import DataParallel\n","from torch.cuda.amp import autocast, GradScaler\n","import torch.nn.functional as F\n","from torchvision.utils import save_image\n","import torchsummary\n","from tqdm import tqdm\n","from models.e2dsr import *\n","import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","import time\n","class ImageDataset(Dataset):\n","    def __init__(self, lr_dir, hr_dir, valid = False, scale=4):\n","        self.lr_files = sorted(os.listdir(lr_dir))\n","        self.hr_files = sorted(os.listdir(hr_dir))\n","        self.lr_dir = lr_dir\n","        self.hr_dir = hr_dir\n","        self.valid = valid\n","        self.scale = scale\n","    def __len__(self):\n","        return len(self.lr_files)\n","\n","    def __getitem__(self, idx):\n","        lr_image = Image.open(os.path.join(self.lr_dir, self.lr_files[idx])).convert('RGB')\n","        hr_image = Image.open(os.path.join(self.hr_dir, self.hr_files[idx])).convert('RGB')\n","        \n","        w, h = hr_image.size\n","        if self.valid:\n","            lr_image = lr_image.resize((w//self.scale, h//self.scale))\n","            \n","        transform = transforms.Compose([\n","            # transforms.ToPILImage(),\n","            transforms.ToTensor()\n","        ])\n","        \n","        lr_image = transform(lr_image)\n","        hr_image = transform(hr_image)\n","        return lr_image, hr_image\n","def calculate_psnr(img1, img2):\n","    mse = torch.mean((img1 - img2) ** 2)\n","    if mse == 0:\n","        return float('inf')\n","    max_pixel = 1.0\n","    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n","    return psnr.item()\n","\n","from torch.amp import autocast, GradScaler\n","scaler = GradScaler()\n","\n","# Khởi tạo dataset và dataloader\n","for scale in [2, 3, 4]:\n","    print(f'Training Scale {scale}')\n","    train_lr_dir = f'dataset/Train/LR_{scale}'\n","    train_hr_dir = 'dataset/Train/HR'\n","    valid_lr_dir = 'dataset/Test/HR'\n","    valid_hr_dir = 'dataset/Test/HR'\n","    \n","    train_dataset = ImageDataset(train_lr_dir, train_hr_dir)\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","    valid_dataset = ImageDataset(valid_lr_dir, valid_hr_dir, valid = True, scale=scale)\n","    valid_loader = DataLoader(valid_dataset)\n","\n","    # Khởi tạo loss function\n","    criterion = nn.MSELoss()\n","    e2dsr_sobel = E2DSR(edge_option='sobel', scale_factor=scale).to(device)\n","    e2dsr_canny = E2DSR(edge_option='canny', scale_factor=scale).to(device)\n","    # Khởi tạo optimizers, schedulers cho từng mô hình\n","    optim_e2dsr_canny = optim.Adam(e2dsr_canny.parameters(), lr=1e-4, betas=(0.9, 0.999))\n","    scheduler_e2dsr_canny = optim.lr_scheduler.StepLR(optim_e2dsr_canny, step_size=10**5, gamma=0.5)\n","\n","    optim_e2dsr_sobel = optim.Adam(e2dsr_sobel.parameters(), lr=1e-4, betas=(0.9, 0.999))\n","    scheduler_e2dsr_sobel = optim.lr_scheduler.StepLR(optim_e2dsr_sobel, step_size=10**5, gamma=0.5)\n","    \n","    num_epochs = 24\n","\n","    best_psnr_e2dsr_canny = float('-inf')\n","    best_psnr_e2dsr_sobel = float('-inf')\n","    torch.cuda.empty_cache()\n","\n","    losses_e2dsr_canny = []\n","    losses_e2dsr_sobel = []\n","\n","    avg_psnr_e2dsr_canny = []\n","    avg_psnr_e2dsr_sobel = []\n","\n","    val_avg_psnr_e2dsr_canny = []\n","    val_avg_psnr_e2dsr_sobel = []\n","\n","    patience = 5\n","    epochs_no_improve = 0\n","    log_file = open('outputs/train_log/e2dsr.txt', 'a')\n","\n","    for epoch in range(num_epochs):\n","        e2dsr_canny.train()\n","        e2dsr_sobel.train()\n","\n","        epoch_loss_e2dsr_canny, psnr_values_e2dsr_canny = 0, 0\n","        epoch_loss_e2dsr_sobel, psnr_values_e2dsr_sobel = 0, 0\n","\n","        start_time = time.time()\n","        torch.cuda.empty_cache()\n","        # Training loop for each model\n","        for i, (lr_images, hr_images) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')):\n","            lr_images = lr_images.to(device)\n","            hr_images = hr_images.to(device)\n","\n","            # # Train e2dsr_canny model\n","            optim_e2dsr_canny.zero_grad()\n","            with autocast(device_type='cuda'):\n","                outputs_e2dsr_canny = e2dsr_canny(lr_images)\n","                loss_e2dsr_canny = criterion(outputs_e2dsr_canny, hr_images)\n","            psnr_e2dsr_canny = calculate_psnr(outputs_e2dsr_canny, hr_images)\n","\n","            scaler.scale(loss_e2dsr_canny).backward()\n","            scaler.step(optim_e2dsr_canny)\n","            scaler.update()\n","            scheduler_e2dsr_canny.step()\n","\n","            epoch_loss_e2dsr_canny += loss_e2dsr_canny.item()\n","            \n","            psnr_values_e2dsr_canny += psnr_e2dsr_canny\n","\n","            # Train e2dsr_sobel model\n","            optim_e2dsr_sobel.zero_grad()\n","            with autocast(device_type='cuda'):\n","                outputs_e2dsr_sobel = e2dsr_sobel(lr_images)\n","                loss_e2dsr_sobel = criterion(outputs_e2dsr_sobel, hr_images)\n","            psnr_e2dsr_sobel = calculate_psnr(outputs_e2dsr_sobel, hr_images)\n","\n","            scaler.scale(loss_e2dsr_sobel).backward()\n","            scaler.step(optim_e2dsr_sobel)\n","            scaler.update()\n","            scheduler_e2dsr_sobel.step()\n","\n","            epoch_loss_e2dsr_sobel += loss_e2dsr_sobel.item()\n","            psnr_values_e2dsr_sobel += psnr_e2dsr_sobel\n","\n","        # Average losses and PSNRs\n","        avg_epoch_loss_e2dsr_canny = epoch_loss_e2dsr_canny / len(train_loader)\n","        avg_psnr_e2dsr_canny_epoch = psnr_values_e2dsr_canny / len(train_loader)\n","        losses_e2dsr_canny.append(avg_epoch_loss_e2dsr_canny)\n","        avg_psnr_e2dsr_canny.append(avg_psnr_e2dsr_canny_epoch)\n","\n","        avg_epoch_loss_e2dsr_sobel = epoch_loss_e2dsr_sobel / len(train_loader)\n","        avg_psnr_e2dsr_sobel_epoch = psnr_values_e2dsr_sobel / len(train_loader)\n","        losses_e2dsr_sobel.append(avg_epoch_loss_e2dsr_sobel)\n","        avg_psnr_e2dsr_sobel.append(avg_psnr_e2dsr_sobel_epoch)\n","\n","        # Validation for all models\n","        e2dsr_canny.eval()\n","        e2dsr_sobel.eval()\n","\n","        val_psnr_e2dsr_canny, val_psnr_e2dsr_sobel = 0, 0\n","        val_psnr_vdsr, val_psnr_fsrcnn = 0, 0\n","\n","        with torch.no_grad():\n","            for (lr_images, hr_images) in valid_loader:\n","                lr_images = lr_images.cuda()\n","                hr_images = hr_images.cuda()\n","\n","                # # Validate e2dsr_canny\n","                outputs_e2dsr_canny = e2dsr_canny(lr_images)\n","                psnr_e2dsr_canny = calculate_psnr(outputs_e2dsr_canny, hr_images)\n","                val_psnr_e2dsr_canny += psnr_e2dsr_canny\n","\n","                # Validate e2dsr_sobelbel\n","                outputs_e2dsr_sobel = e2dsr_sobel(lr_images)\n","                psnr_e2dsr_sobel = calculate_psnr(outputs_e2dsr_sobel, hr_images)\n","                val_psnr_e2dsr_sobel += psnr_e2dsr_sobel\n","\n","        val_avg_psnr_e2dsr_canny_epoch = val_psnr_e2dsr_canny / len(valid_loader)\n","        val_avg_psnr_e2dsr_canny.append(val_avg_psnr_e2dsr_canny_epoch)\n","\n","        val_avg_psnr_e2dsr_sobel_epoch = val_psnr_e2dsr_sobel / len(valid_loader)\n","        val_avg_psnr_e2dsr_sobel.append(val_avg_psnr_e2dsr_sobel_epoch)\n","\n","        # Save best model\n","        if val_avg_psnr_e2dsr_canny_epoch > best_psnr_e2dsr_canny:\n","            best_psnr_e2dsr_canny = val_avg_psnr_e2dsr_canny_epoch\n","            torch.save(e2dsr_canny.state_dict(), f'outputs/weight_sr/x{scale}/best_e2dsr_canny.pth')\n","            print(f\"Saved e2dsr_cannyR model with PSNR {best_psnr_e2dsr_canny:.4f}\")\n","        if val_avg_psnr_e2dsr_sobel_epoch > best_psnr_e2dsr_sobel:\n","            best_psnr_e2dsr_sobel = val_avg_psnr_e2dsr_sobel_epoch\n","            torch.save(e2dsr_sobel.state_dict(), f'outputs/weight_sr/x{scale}/best_e2dsr_sobel.pth')\n","            print(f\"Saved e2dsr_sobel model with PSNR {best_psnr_e2dsr_sobel:.4f}\")\n","\n","        torch.save(e2dsr_canny.state_dict(), f'outputs/path/x{scale}/e2dsr_canny_{epoch}.pth')\n","        torch.save(e2dsr_sobel.state_dict(), f'outputs/path/x{scale}/e2dsr_sobel_{epoch}.pth')\n","\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] completed: e2dsr_canny Loss: {avg_epoch_loss_e2dsr_canny:.4f}, PSNR: {avg_psnr_e2dsr_canny_epoch:.4f}, Validation PSNR: {val_avg_psnr_e2dsr_canny_epoch:.4f},\"\n","              f\"e2dsr_sobel Loss: {avg_epoch_loss_e2dsr_sobel:.4f}, PSNR: {avg_psnr_e2dsr_sobel_epoch:.4f}, Validation PSNR: {val_avg_psnr_e2dsr_sobel_epoch:.4f}\")\n","    # \n","        log_file.write(f\"Epoch {epoch+1}:  e2dsr_canny PSNR: {avg_psnr_e2dsr_canny_epoch:.4f}, Validation PSNR: {val_avg_psnr_e2dsr_canny_epoch:.4f}\\n\")\n","        log_file.write(f\"              e2dsr_sobel PSNR: {avg_psnr_e2dsr_sobel_epoch:.4f}, Validation PSNR: {val_avg_psnr_e2dsr_sobel_epoch:.4f}\\n\")\n","\n","        log_file.flush()\n","\n","    log_file.close()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# SRResNEt EDSR"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","from PIL import Image\n","import shutil\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.parallel import DataParallel\n","from torch.cuda.amp import autocast, GradScaler\n","import torch.nn.functional as F\n","from torchvision.utils import save_image\n","import torchsummary\n","from tqdm import tqdm\n","from models.srresnet import *\n","from models.edsr import *\n","\n","import time\n","import os\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","class ImageDataset(Dataset):\n","    def __init__(self, lr_dir, hr_dir, valid = False, scale=4):\n","        self.lr_files = sorted(os.listdir(lr_dir))\n","        self.hr_files = sorted(os.listdir(hr_dir))\n","        self.lr_dir = lr_dir\n","        self.hr_dir = hr_dir\n","        self.valid = valid\n","        self.scale = scale\n","    def __len__(self):\n","        return len(self.lr_files)\n","\n","    def __getitem__(self, idx):\n","        lr_image = Image.open(os.path.join(self.lr_dir, self.lr_files[idx])).convert('RGB')\n","        hr_image = Image.open(os.path.join(self.hr_dir, self.hr_files[idx])).convert('RGB')\n","        \n","        w, h = hr_image.size\n","        if self.valid:\n","            lr_image = lr_image.resize((w//self.scale, h//self.scale))\n","            \n","        transform = transforms.Compose([\n","            # transforms.ToPILImage(),\n","            transforms.ToTensor()\n","        ])\n","        \n","        lr_image = transform(lr_image)\n","        hr_image = transform(hr_image)\n","        return lr_image, hr_image\n","    \n","def calculate_psnr(img1, img2):\n","    mse = torch.mean((img1 - img2) ** 2)\n","    if mse == 0:\n","        return float('inf')\n","    max_pixel = 1.0\n","    psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n","    return psnr.item()\n","\n","from torch.amp import autocast, GradScaler\n","scaler = GradScaler()\n","\n","# Khởi tạo dataset và dataloader\n","for scale in [2, 3, 4]:\n","    print(f'Traing scale {scale}')\n","    train_lr_dir = f'dataset/Train/LR_{scale}'\n","    train_hr_dir = 'dataset/Train/HR'\n","    valid_lr_dir = 'dataset/Test/HR'\n","    valid_hr_dir = 'dataset/Test/HR'\n","    train_dataset = ImageDataset(train_lr_dir, train_hr_dir, scale=scale)\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","    valid_dataset = ImageDataset(valid_lr_dir, valid_hr_dir, scale=scale, valid = True)\n","    valid_loader = DataLoader(valid_dataset)\n","\n","    # Khởi tạo loss function\n","    criterion = nn.MSELoss()\n","    edsr = EDSR(scale=scale).to(device)\n","    srresnet = SRResNet(scale=scale).to(device)\n","    \n","    # Khởi tạo optimizers, schedulers cho từng mô hình\n","    optim_edsr = optim.Adam(edsr.parameters(), lr=1e-4, betas=(0.9, 0.999))\n","    scheduler_edsr = optim.lr_scheduler.StepLR(optim_edsr, step_size=10**5, gamma=0.5)\n","\n","    optim_srresnet = optim.Adam(srresnet.parameters(), lr=1e-4, betas=(0.9, 0.999))\n","    scheduler_srresnet = optim.lr_scheduler.StepLR(optim_srresnet, step_size=10**5, gamma=0.5)\n","    \n","    num_epochs = 24\n","\n","    best_psnr_edsr = float('-inf')\n","    best_psnr_srresnet = float('-inf')\n","\n","    torch.cuda.empty_cache()\n","\n","    losses_edsr = []\n","    losses_srresnet = []\n","\n","    avg_psnr_edsr = []\n","    avg_psnr_srresnet = []\n","\n","    val_avg_psnr_edsr = []\n","    val_avg_psnr_srresnet = []\n","\n","    patience = 5\n","    epochs_no_improve = 0\n","    log_file = open('outputs/train_log/edsr_srresnet.txt', 'a')\n","\n","    for epoch in range(num_epochs):\n","        edsr.train()\n","        srresnet.train()\n","\n","        epoch_loss_edsr, psnr_values_edsr = 0, 0\n","        epoch_loss_srresnet, psnr_values_srresnet = 0, 0\n","\n","        start_time = time.time()\n","        torch.cuda.empty_cache()\n","        # Training loop for each model\n","        for i, (lr_images, hr_images) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')):\n","            lr_images = lr_images.to(device)\n","            hr_images = hr_images.to(device)\n","\n","            # # Train EDSR model\n","            optim_edsr.zero_grad()\n","            with autocast(device_type='cuda'):\n","                outputs_edsr = edsr(lr_images)\n","                loss_edsr = criterion(outputs_edsr, hr_images)\n","            psnr_edsr = calculate_psnr(outputs_edsr, hr_images)\n","\n","            scaler.scale(loss_edsr).backward()\n","            scaler.step(optim_edsr)\n","            scaler.update()\n","            scheduler_edsr.step()\n","\n","            epoch_loss_edsr += loss_edsr.item()\n","            \n","            psnr_values_edsr += psnr_edsr\n","            # Train SRResNet model\n","            optim_srresnet.zero_grad()\n","            with autocast(device_type='cuda'):\n","                outputs_srresnet = srresnet(lr_images)\n","                loss_srresnet = criterion(outputs_srresnet, hr_images)\n","            psnr_srresnet = calculate_psnr(outputs_srresnet, hr_images)\n","\n","            scaler.scale(loss_srresnet).backward()\n","            scaler.step(optim_srresnet)\n","            scaler.update()\n","            scheduler_srresnet.step()\n","\n","            epoch_loss_srresnet += loss_srresnet.item()\n","            psnr_values_srresnet += psnr_srresnet\n","\n","\n","        # Average losses and PSNRs\n","        avg_epoch_loss_edsr = epoch_loss_edsr / len(train_loader)\n","        avg_psnr_edsr_epoch = psnr_values_edsr / len(train_loader)\n","        losses_edsr.append(avg_epoch_loss_edsr)\n","        avg_psnr_edsr.append(avg_psnr_edsr_epoch)\n","\n","        avg_epoch_loss_srresnet = epoch_loss_srresnet / len(train_loader)\n","        avg_psnr_srresnet_epoch = psnr_values_srresnet / len(train_loader)\n","        losses_srresnet.append(avg_epoch_loss_srresnet)\n","        avg_psnr_srresnet.append(avg_psnr_srresnet_epoch)\n","\n","        # Validation for all models\n","        edsr.eval()\n","        srresnet.eval()\n","    \n","\n","        val_psnr_edsr, val_psnr_srresnet = 0, 0\n","\n","        with torch.no_grad():\n","            for (lr_images, hr_images) in valid_loader:\n","                lr_images = lr_images.cuda()\n","                hr_images = hr_images.cuda()\n","\n","                # # Validate EDSR\n","                outputs_edsr = edsr(lr_images)\n","                psnr_edsr = calculate_psnr(outputs_edsr, hr_images)\n","                val_psnr_edsr += psnr_edsr\n","\n","                # Validate SRResNet\n","                outputs_srresnet = srresnet(lr_images)\n","                psnr_srresnet = calculate_psnr(outputs_srresnet, hr_images)\n","                val_psnr_srresnet += psnr_srresnet\n","\n","        val_avg_psnr_edsr_epoch = val_psnr_edsr / len(valid_loader)\n","        val_avg_psnr_edsr.append(val_avg_psnr_edsr_epoch)\n","\n","        val_avg_psnr_srresnet_epoch = val_psnr_srresnet / len(valid_loader)\n","        val_avg_psnr_srresnet.append(val_avg_psnr_srresnet_epoch)\n","\n","    \n","        # Save best model\n","        if val_avg_psnr_edsr_epoch > best_psnr_edsr:\n","            best_psnr_edsr = val_avg_psnr_edsr_epoch\n","            torch.save(edsr.state_dict(), f'outputs/weight_sr/x{scale}/best_edsr.pth')\n","            print(f\"Saved EDSRR model with PSNR {best_psnr_edsr:.4f}\")\n","        if val_avg_psnr_srresnet_epoch > best_psnr_srresnet:\n","            best_psnr_srresnet = val_avg_psnr_srresnet_epoch\n","            torch.save(srresnet.state_dict(), f'outputs/weight_sr/x{scale}/best_srresnet.pth')\n","            print(f\"Saved SRResNet model with PSNR {best_psnr_srresnet:.4f}\")\n","\n","        torch.save(edsr.state_dict(), f'outputs/path/x{scale}/edsr_{epoch}.pth')\n","        torch.save(srresnet.state_dict(), f'outputs/path/x{scale}/srresnet_{epoch}.pth')\n","        \n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] completed: EDSR Loss: {avg_epoch_loss_edsr:.4f}, PSNR: {avg_psnr_edsr_epoch:.4f}, Validation PSNR: {val_avg_psnr_edsr_epoch:.4f},\"\n","            f\"SRResNEt Loss: {avg_epoch_loss_srresnet:.4f}, PSNR: {avg_psnr_srresnet_epoch:.4f}, Validation PSNR: {val_avg_psnr_srresnet_epoch:.4f}\")\n","\n","        log_file.write(f\"Epoch {epoch+1}:  EDSR PSNR: {avg_psnr_edsr_epoch:.4f}, Validation PSNR: {val_avg_psnr_edsr_epoch:.4f}\\n\")\n","        log_file.write(f\"              SRResNet PSNR: {avg_psnr_srresnet_epoch:.4f}, Validation PSNR: {val_avg_psnr_srresnet_epoch:.4f}\\n\")\n","        \n","        log_file.flush()\n","\n","    log_file.close()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# SRCNN "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","import numpy as np\n","import os\n","from PIL import Image\n","import shutil\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, Dataset\n","from torch.nn.parallel import DataParallel\n","from torch.cuda.amp import autocast, GradScaler\n","import torch.nn.functional as F\n","from torchvision.utils import save_image\n","\n","from tqdm import tqdm\n","from models.vdsr import *\n","from models.srcnn import *\n","import time\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class ImageDataset(Dataset):\n","    def __init__(self, lr_dir, hr_dir, valid = False, scale=4, vdsr = False):\n","        self.lr_files = sorted(os.listdir(lr_dir))\n","        self.hr_files = sorted(os.listdir(hr_dir))\n","        self.lr_dir = lr_dir\n","        self.hr_dir = hr_dir\n","        self.valid = valid\n","        self.scale = scale\n","        self.vdsr = vdsr\n","    def __len__(self):\n","        return len(self.lr_files)\n","\n","    def __getitem__(self, idx):\n","        lr_image = Image.open(os.path.join(self.lr_dir, self.lr_files[idx])).convert('RGB')\n","        hr_image = Image.open(os.path.join(self.hr_dir, self.hr_files[idx])).convert('RGB')\n","        \n","        w, h = hr_image.size\n","  \n","        if self.valid:\n","            lr_image = lr_image.resize((w//self.scale, h//self.scale))\n","        if self.vdsr:\n","            lr_image = lr_image.resize((w, h))\n","        transform = transforms.Compose([\n","            # transforms.ToPILImage(),\n","            transforms.ToTensor()\n","        ])\n","        \n","        lr_image = transform(lr_image)\n","        hr_image = transform(hr_image)\n","        return lr_image, hr_image\n","from torch.amp import autocast, GradScaler\n","scaler = GradScaler()\n","\n","# Khởi tạo dataset và dataloader\n","for scale in [2, 3, 4]:\n","    print(f'train scale {scale}')\n","    train_lr_dir = f'dataset/Train/LR_{scale}'\n","    train_hr_dir = 'dataset/Train/HR'\n","    valid_lr_dir = 'dataset/Test/HR'\n","    valid_hr_dir = 'dataset/Test/HR'\n","    vdsr = VDSR().to(device)\n","    srcnn = SRCNN().to(device)\n","    train_dataset = ImageDataset(train_lr_dir, train_hr_dir,scale=scale, vdsr=True)\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n","    valid_dataset = ImageDataset(valid_lr_dir, valid_hr_dir, valid=True, scale=scale,vdsr=True)\n","    valid_loader = DataLoader(valid_dataset)\n","\n","    # Khởi tạo loss function\n","    criterion = nn.MSELoss()\n","    \n","    optim_srcnn = optim.Adam(srcnn.parameters(), lr=1e-5, betas=(0.9, 0.999))\n","    scheduler_srcnn = optim.lr_scheduler.StepLR(optim_srcnn, step_size=10**5, gamma=0.5)\n","\n","    optim_vdsr = optim.Adam(vdsr.parameters(), lr=1e-4, betas=(0.9, 0.999))\n","    scheduler_vdsr = optim.lr_scheduler.StepLR(optim_vdsr, step_size=10**5, gamma=0.5)\n","    # Hàm tính PSNR\n","    def calculate_psnr(img1, img2):\n","        mse = torch.mean((img1 - img2) ** 2)\n","        if mse == 0:\n","            return float('inf')\n","        max_pixel = 1.0\n","        psnr = 20 * torch.log10(max_pixel / torch.sqrt(mse))\n","        return psnr.item()\n","\n","    num_epochs = 24\n","\n","    best_psnr_srcnn = float('-inf')\n","    best_psnr_vdsr = float('-inf')\n","    torch.cuda.empty_cache()\n","\n","    losses_srcnn = []\n","    losses_vdsr = []\n","\n","    avg_psnr_srcnn = []\n","    avg_psnr_vdsr = []\n","\n","    val_avg_psnr_srcnn = []\n","    val_avg_psnr_vdsr = []\n","\n","    patience = 5\n","    epochs_no_improve = 0\n","    log_file = open('outputs/train_log/srcnnn_vdsr.txt', 'a')\n","\n","    for epoch in range(num_epochs):\n","        srcnn.train()\n","        vdsr.train()\n","\n","        epoch_loss_srcnn, psnr_values_srcnn = 0, 0\n","        epoch_loss_vdsr, psnr_values_vdsr = 0, 0\n","        start_time = time.time()\n","\n","        # Training loop for srcnn\n","        for (lr_images, hr_images) in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch'):\n","            lr_images = lr_images.cuda()\n","            hr_images = hr_images.cuda()\n","\n","            # Train srcnn model\n","            optim_srcnn.zero_grad()\n","            with autocast(device_type='cuda'):\n","                outputs_srcnn =srcnn(lr_images)\n","                loss_srcnn = criterion(outputs_srcnn, hr_images)\n","            psnr_srcnn = calculate_psnr(outputs_srcnn, hr_images)\n","            # if psnr_srcnn < 27:\n","            scaler.scale(loss_srcnn).backward()\n","            scaler.step(optim_srcnn)\n","            scaler.update()\n","            scheduler_srcnn.step()\n","\n","            epoch_loss_srcnn += loss_srcnn.item()\n","            psnr_values_srcnn += psnr_srcnn\n","\n","            optim_vdsr.zero_grad()\n","            with autocast(device_type='cuda'):\n","                outputs_vdsr = vdsr(lr_images)\n","                loss_vdsr = criterion(outputs_vdsr, hr_images)\n","            psnr_vdsr = calculate_psnr(outputs_vdsr, hr_images)\n","\n","            scaler.scale(loss_vdsr).backward()\n","            scaler.step(optim_vdsr)\n","            scaler.update()\n","            scheduler_vdsr.step()\n","\n","            epoch_loss_vdsr += loss_vdsr.item()\n","            psnr_values_vdsr += psnr_vdsr\n","        \n","        # Training loop for vdsr\n","    \n","            \n","        # Average losses and PSNRs\n","        avg_epoch_loss_srcnn = epoch_loss_srcnn / len(train_loader)\n","        avg_psnr_srcnn_epoch = psnr_values_srcnn / len(train_loader)\n","        losses_srcnn.append(avg_epoch_loss_srcnn)\n","        avg_psnr_srcnn.append(avg_psnr_srcnn_epoch)\n","\n","        avg_epoch_loss_vdsr = epoch_loss_vdsr / len(train_loader)\n","        avg_psnr_vdsr_epoch = psnr_values_vdsr / len(train_loader)\n","        losses_vdsr.append(avg_epoch_loss_vdsr)\n","        avg_psnr_vdsr.append(avg_psnr_vdsr_epoch)\n","\n","        # Validation for srcnn and vdsr\n","        srcnn.eval()\n","        vdsr.eval()\n","\n","        val_psnr_srcnn, val_psnr_vdsr = 0, 0\n","\n","        with torch.no_grad():\n","            # Validate srcnn\n","            for (lr_images, hr_images) in valid_loader:\n","                lr_images = lr_images.cuda()\n","                hr_images = hr_images.cuda()\n","\n","                outputs_srcnn = srcnn(lr_images)\n","                psnr_srcnn = calculate_psnr(outputs_srcnn, hr_images)\n","                val_psnr_srcnn += psnr_srcnn\n","\n","\n","                outputs_vdsr = vdsr(lr_images)\n","                psnr_vdsr = calculate_psnr(outputs_vdsr, hr_images)\n","                val_psnr_vdsr += psnr_vdsr\n","            # Validate vdsr\n","            \n","\n","\n","        val_avg_psnr_srcnn_epoch = val_psnr_srcnn / len(valid_loader)\n","        val_avg_psnr_srcnn.append(val_avg_psnr_srcnn_epoch)\n","\n","        val_avg_psnr_vdsr_epoch = val_psnr_vdsr / len(valid_loader)\n","        val_avg_psnr_vdsr.append(val_avg_psnr_vdsr_epoch)\n","\n","        # Save best model for srcnn\n","        if val_avg_psnr_srcnn_epoch > best_psnr_srcnn:\n","            best_psnr_srcnn = val_avg_psnr_srcnn_epoch\n","            torch.save(srcnn.state_dict(), f'outputs/weight_sr/x{scale}/best_srcnn.pth')\n","            print(f\"Saved SRCNN model with PSNR {best_psnr_srcnn:.4f}\")\n","        # Save best model for vdsr\n","        if val_avg_psnr_vdsr_epoch > best_psnr_vdsr:\n","            best_psnr_vdsr = val_avg_psnr_vdsr_epoch\n","            torch.save(vdsr.state_dict(), f'outputs/weight_sr/x{scale}/best_vdsr.pth')\n","            print(f\"Saved VDSR model with PSNR {best_psnr_vdsr:.4f}\")\n","\n","        torch.save(srcnn.state_dict(), f'outputs/path/srcnn_{epoch+10}.pth')\n","        torch.save(vdsr.state_dict(), f'outputs/path/vdsr_{epoch+10}.pth')\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] completed: srcnn Loss: {avg_epoch_loss_srcnn:.4f}, PSNR: {avg_psnr_srcnn_epoch:.4f}, Validation PSNR: {val_avg_psnr_srcnn_epoch:.4f}\")\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] completed: vdsr Loss: {avg_epoch_loss_vdsr:.4f}, PSNR: {avg_psnr_vdsr_epoch:.4f}, Validation PSNR: {val_avg_psnr_vdsr_epoch:.4f}\")\n","\n","        log_file.write(f\"Epoch {epoch+1}: WDSRA PSNR: {avg_psnr_srcnn_epoch:.4f}, Validation PSNR: {val_avg_psnr_srcnn_epoch:.4f}\\n\")\n","        log_file.write(f\"Epoch {epoch+1}: vdsr PSNR: {avg_psnr_vdsr_epoch:.4f}, Validation PSNR: {val_avg_psnr_vdsr_epoch:.4f}\\n\")\n","\n","        # log_file.flush()\n","\n","    log_file.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5414450,"sourceId":8989742,"sourceType":"datasetVersion"},{"datasetId":5671931,"sourceId":9356096,"sourceType":"datasetVersion"},{"datasetId":5764801,"sourceId":9478075,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
